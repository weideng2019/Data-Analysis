---
title: "Diamond project"
author: "Wei Deng"
date: "February 14, 2019"
output:
  html_document: default
  pdf_document: default
---
# Load Libraries
```{r}
library(dplyr)
library(rpart)
library(ggplot2)
library(corrplot)
library(randomForest)
library(tree)
library(neuralnet)
```
```{r}
library(xgboost)
library(caret)
library(modelr)
library(rpart)

```


# Load data
## The original diamond data is loaded

```{r}
setwd("E://study//TAMU//685//practice using R")
diamond_raw=read.csv("main_diamond.csv")
```

# View the data
## head of data

```{r}
head(diamond_raw)
```

## summarize the data

```{r}
summary(diamond_raw)
```

## check the structure of the data

```{r}
str(diamond_raw)
```

# Exploratory Data Anlysis
## scatter plot matrix

```{r, echo=FALSE}
plot(diamond_raw[,-1])
```

## The counts for cut

```{r, echo=FALSE}
## speficy the order of boxplot
diamond_raw$cut<-factor(diamond_raw$cut, levels=c("Fair","Good","Very Good","Premium","Ideal"))
ggplot(data=diamond_raw)+geom_bar(mapping = aes(x=cut,fill=cut))+
  theme_classic()
```
## The counts for clarity

```{r, echo=FALSE}
diamond_raw$clarity<-factor(diamond_raw$clarity, levels=c("I1","SI1","SI2","VS1","VS2","VVS1","VVS2","IF"))
ggplot(data=diamond_raw)+geom_bar(mapping = aes(x=clarity,fill=clarity))+
  theme_classic()
```

## The counts for color

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_bar(mapping = aes(x=color,fill=color))+
  theme_classic()
```
## The relationship between carat and price

```{r, echo=FALSE}
ggplot(data=diamond_raw, mapping=aes(x=cut, y=price))+
  geom_boxplot() +
  theme_classic()

```
```{r, echo=FALSE}
ggplot(data=diamond_raw, aes(cut, price))+
  geom_boxplot() +
  theme_classic()

```

## The relationship between clarity and price

```{r, echo=FALSE}
ggplot(data=diamond_raw, mapping=aes(x=clarity, y=price))+
  geom_boxplot() +
  theme_classic()

```
## The relationship between color and price

```{r, echo=FALSE}
ggplot(data=diamond_raw, mapping=aes(x=color, y=price))+
  geom_boxplot() +
  theme_classic()

```
## The relationship between carat and price

```{r, echo=FALSE}
par(mfrow=c(1,3))
ggplot(data=diamond_raw)+
  geom_point(mapping=aes(x=carat, y=price,color=cut)) +
  theme_classic()
ggplot(data=diamond_raw)+
  geom_point(mapping=aes(x=carat, y=price,color=color)) +
  theme_classic()
ggplot(data=diamond_raw)+
  geom_point(mapping=aes(x=carat, y=price,color=clarity)) +
  theme_classic()
```

## The distribution of carat

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=carat), binwidth = 0.5)+
  
  theme_classic()
```

## Check the counts in each bin
### This information would be used for data cleaning. 
```{r}
diamond_raw %>%
  count(cut_width(carat, 0.5))
```

## The distribution of x

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=x), binwidth = 0.5)+
  theme_classic()
```
## Check the counts for each bin
### This imformation would be used for data cleaning

```{r}
diamond_raw %>%
  count(cut_width(x,0.5))
```

## The distribution of y

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=y), binwidth = 0.5)+
  theme_classic()
```

### zoom into see the unusual values of y

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=y), binwidth = 0.5)+
  coord_cartesian(ylim=c(0,50))+
  theme_classic()

```




## Check the counts in each bin
### This imformation would be used for data cleaning
```{r}
diamond_raw %>%
  count(cut_width(y,0.5))
```

## The distribution of z
```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=z), binwidth =0.5)+
  theme_classic()
```
### zoom into see the unusual values of z

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=z), binwidth = 0.5)+
  coord_cartesian(ylim=c(0,50))+
  theme_classic()

```

## Check the counts in each bin
### This imformation would be used for data cleaning
```{r}
diamond_raw %>%
  count(cut_width(z,0.5))
```

## The distribution of table
```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=table), binwidth =2)+
  theme_classic()
```

### zoom into see the unusual values of table

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=table), binwidth = 2)+
  coord_cartesian(ylim=c(0,50))+
  theme_classic()

```


## Check the counts in each bin
### This imformation would be used for data cleaning

```{r}
diamond_raw %>%
  count(cut_width(table,2))
```
## The distribution of depth

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=depth), binwidth =2)+
  theme_classic()

```
### zoom into see the unusual values of table

```{r, echo=FALSE}
ggplot(data=diamond_raw)+geom_histogram(mapping = aes(x=depth), binwidth = 2)+
  coord_cartesian(ylim=c(0,50))+
  theme_classic()

```

## Check counts in each bin
### This information would be used for data cleaning

```{r}
diamond_raw %>%
  count(cut_width(depth,2))
```

# Data clean and pre-processing
## According to the distributions of carat, x, y, table and depth, the ranges for these five variables were set as below:
## carat:(0.2,4)
## x:(3.9, 8.6)
## y:(3.9,8.5)
## z:(2.4,5.3)
## table:(52,66)
## depth:(57,67)

```{r}
diamond<-diamond_raw %>%
         filter(between(carat, 0.2, 4))
diamond<-diamond %>%
         filter(between(x, 3.9,8.6))
diamond<- diamond %>%
         filter(between(y, 3.9,8.5))
diamond<- diamond %>%
         filter(between(z, 2.4, 5.3)) 
diamond<- diamond %>%
         filter(between(table, 52, 66)) 
diamond<- diamond%>%
         filter(between(depth, 57, 67))

```

# Create new variables for data analysis
## create three new ordinary vairables corresponding to cut, color and clarity
### cut: Fair=1, Good=2, Very Good=3, Premium=4, Ideal=5
### clarity: I1=1, SI2=2, SI1=3, VS2=4, VS1=5, VVS2=6, VVS1=7, IF=8
### color: D=1, E=2, F=3, G=4,H=5,I=6,J=7

```{r}
cut_2<-rep(0, length(diamond[,'cut']))
i=1
for (i in 1: 47762){
  if (diamond[i,'cut']=="Fair")
    cut_2[i] = 1
  else if (diamond[i,'cut']=="Good")
    cut_2[i] = 2
  else if (diamond[i,'cut']=="Very Good")
    cut_2[i] = 3
  else if (diamond[i,'cut']=="Premiun")
    cut_2[i] = 4
  else if (diamond[i,'cut']=="Ideal")
    cut_2[i]= 5
  
}

```
```{r}
color_2<-rep(0, length(diamond[,'color']))
i=1
for (i in 1: 47762){
  if (diamond[i,'color']=="D")
    color_2[i] = 1
  else if (diamond[i,'color']=="E")
    color_2[i] = 2
  else if (diamond[i,'color']=="F")
    color_2[i] = 3
  else if (diamond[i,'color']=="G")
    color_2[i] = 4
  else if (diamond[i,'color']=="H")
    color_2[i]= 5
  else if (diamond[i,'color']=="I")
    color_2[i]= 6
  else if (diamond[i,'color']=="J")
    color_2[i]= 7
}

```
```{r}
clarity_2<-rep(0, length(diamond[,'clarity']))
i=1
for (i in 1: 47762){
  if (diamond[i,'clarity']=="I1")
    clarity_2[i] = 1
  else if (diamond[i,'clarity']=="SI2")
    clarity_2[i] = 2
  else if (diamond[i,'clarity']=="SI1")
    clarity_2[i] = 3
  else if (diamond[i,'clarity']=="VS2")
    clarity_2[i] = 4
  else if (diamond[i,'clarity']=="VS1")
    clarity_2[i]= 5
  else if (diamond[i,'clarity']=="VVS2")
    clarity_2[i]= 6
  else if (diamond[i,'clarity']=="VVS1")
    clarity_2[i]= 7
  else if (diamond[i,'clarity']=="IF")
    clarity_2[i]= 8
}
```

## Mutate the tree new columns to diamond dataset

```{r}
diamond<-mutate(diamond, cut_2)
diamond<-mutate(diamond, color_2)
diamond<-mutate(diamond, clarity_2)
```

## Create two index to according to carat vs id plot
### From the carat vs id plot, there seems at least two populations seperated by the values of id or carat
```{r, echo=FALSE}
ggplot(data=diamond, mapping=aes(x=id, y=carat))+
geom_point() +
  theme_classic()
```

### further plot the price vs id, colored by carat, at least two populations appears on the plot.
```{r, echo=FALSE}
ggplot(data=diamond, mapping=aes(x=id, y=price, color=carat))+
geom_point() +
  
  theme_classic()

```

### By zooming the plot shown above, id=27750 is the breakpoint to seperate data into two populations.
### carat =0.49 as an another breakpoint to seperate all data points into two populations.
```{r}
carat_0.49=as.factor(ifelse(diamond$carat>0.49, "Yes", "No"))
diamond <-data.frame(diamond, carat_0.49)
diamond$carat_0.49=as.factor(diamond$carat_0.49)

```
```{r, echo=0}
ggplot(data=diamond)+
geom_point(mapping=aes(x=id, y=price, color=carat_0.49)) +
  geom_vline(xintercept=27750,col='red')+
  annotate('text', x=27750, y=10000,angle=90, label="id=27750",sep='', vjust=1.5)+
  
  theme_classic()


```
### create index_1 and index_2
### If id < 27,751, index_1 = 1, otherwise index_1 = 0
### If id < 27,751 and carat >0.49,  index_2 = 1, otherwise index_2 = 0


```{r}
index_1<- rep(0, 47762)
index_2<-rep(0, 47762)
i=1
j=1
for (i in 1: 47762){
  if (diamond[i,'id'] < 27751)
    index_1[i] = 1
  else
    index_1[i] = 0
}

for (j in 1: 47762){
  if (diamond[j,'id'] < 27751 & diamond[j, 'carat']>0.49)
    index_2[j] = 1
  else
    index_2[i] = 0
}

```

## Mutate the two index varaibles into the dataset
```{r}
diamond<-mutate(diamond, index_1)
diamond<-mutate(diamond, index_2)
```

## Plot price vs id, colored by index_1

```{r, echo=0}
diamond2 <-data.frame(diamond, index_1)
diamond2$index_1=as.factor(diamond$index_1)
ggplot(data=diamond2)+
geom_point(mapping=aes(x=id, y=price, color=index_1)) +
  
  theme_classic()
```

## Plot price vs id, colored by index_2

```{r, echo=0}
diamond3 <-data.frame(diamond, index_2)
diamond3$index_2=as.factor(diamond$index_2)
ggplot(data=diamond3)+
geom_point(mapping=aes(x=id, y=price, color=index_2)) +
  
  theme_classic()
```


## dept was calculated from x,y, z by the formula: depth=z/mean(x,y), depth only should give the enough imformation from x, y and z
## x,y,z are higly multicollinearity, so it's better to drop x, y and z from the dataset

```{r}
plot(diamond[, c('price',"x", "y", "z")])

```

## Drop x,y,z; cut, color, and clarity; carat_0.49

```{r}
diamond<-diamond %>%
 select(-c(x,y,z) )%>%
  select_if(is.numeric) 
```

## Since the nonlinear relationship between price and carat as plotted before, I would take log_transformation into consideration during modeling.
### create two vairables---ln_price and ln_carat

```{r}
ln_price=log2(diamond$price)
ln_carat=log2(diamond$carat)
diamond<-mutate(diamond,ln_price, ln_carat)
```

## Check the structure of the cleaned and pre-precessed data

```{r}
str(diamond)
```

# Data Modeling
## I'd like to try model diamond data in 4 groups
### Group 1  dependent variable: price       predictors: carat, index_1,table, depth, cut_2, clarity_2 and color_2
### Group 2  dependent variable: price       predictors: carat, index_2,table, depth, cut_2, clarity_2 and color_2
### Group 3  dependent variable: ln_price    predictors: in_carat, index_1,table, depth, cut_2, clarity_2 and color_2
### Group 4  dependent variable: ln_price    predictors: in_carat, index_1,table, depth, cut_2, clarity_2 and color_2

## split data into training and testing datasets

```{r}
set.seed(1000)
train_index<-sample(1:nrow(diamond), nrow(diamond)*0.7)
train<-diamond[train_index,]
test<-diamond[-train_index,]

```
```{r}
dim(train)
dim(test)
dim(diamond)
```

## Group 1
## Decision Tree
```{r}
## fuction to get the maximum average error for tree models evaluation
mae_values<-function(maxdepth, target, predictors, training_data, testing_data){
  predictors <- paste(predictors, collapse="+")
    formula <- as.formula(paste(target,"~",predictors,sep = ""))
    tree_model <- rpart(formula, data = training_data,
                   control = rpart.control(maxdepth = maxdepth))
    # get the mae
    mae <- mae(tree_model, testing_data)
    return(mae)
}
```

```{r}
target<-'price'
predictors<-c('carat', 'index_1','table', 'depth', 'cut_2', 'clarity_2', 'color_2')
## for loop to get MAE between maxdepth=1 andmaxdepth=10
for (i in 1:10){
  mae <- mae_values(maxdepth = i, target = target, predictors = predictors,
                  training_data = train, testing_data =test)
      print(glue::glue("Maxdepth: ",i,"\t MAE: ",mae))
}
  

```

## Random Forests

```{r}

### fit a basic randomforest model, the default value of mtry=round(#number of predictors/3)
rdf.model<-randomForest(price~carat+index_1+table+depth+cut_2+clarity_2+color_2, ntree=50, data=train)
```
```{r}
## check the model details
rdf.model
```
```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```

### Use caret to tune the randomforest model
```{r}
target<-train$price
predictors<-train %>%
  select(c('carat', 'index_1','table', 'depth', 'cut_2', 'clarity_2','color_2'))%>%
  as.matrix()
tuned_rdf.model<-train(x=predictors, y=target, ntree=50, method='rf', data=train)
print(tuned_rdf.model)

```

### check the tuned radnomforest model with mtry=4 and ntree=50
```{r}
rdf.model<-randomForest(price~carat+index_1+table+depth+cut_2+clarity_2+color_2, ntree=50, mtry=4,data=train)
```
```{r}
## check the model details
rdf.model
```
```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```

## Plot the importance of predictors
```{r}
imp<-importance(rdf.model)
imp_table<-data.frame(Variables=row.names(imp),imp_index=round(imp[, 'IncNodePurity'],5))

```
```{r imp_table, echo=FALSE}
ggplot(imp_table, aes(x = reorder(Variables, imp_index), 
                           y = imp_index, fill = imp_index)) +
  geom_bar(stat='identity') + 
  labs(x = 'Variables') +
  coord_flip() + 
  theme_classic()
```

## XGBoost modeling
```{r}

train_1_x<-train%>% select(-c('index_2','price','id','ln_carat','ln_price'))
train_1_x<-as.matrix(train_1_x)
test_1_x<-test%>%select(-c('index_2','price','id','ln_carat','ln_price'))
test_1_x<-as.matrix(test_1_x)
dtrain<-xgb.DMatrix(data=train_1_x, label=train$price)
dtest<-xgb.DMatrix(data=test_1_x, label=test$price)

```

### XGBoost parameters
```{r}
xgb_params<-list(colsample_bytree = 0.7,
                 subsample = 0.7,
                 booster = "gbtree",
                  max_depth = 5,
                 eta = 0.1,
                 eval_metric = "rmse",
                 objective = "reg:linear",
                  gamma = 0)
```

```{r}
watchlist<-list(train=dtrain, test=dtest)
```

### train xgboot model
```{r}
xgb_model<-xgb.train(xgb_params, dtrain, nrounds=200, watchlist=watchlist)
```

### plot train_RMSE/test_RMSE vs interaction

```{r}
# xgb_model ## call the model
errors<-data.frame(xgb_model$evaluation_log)
#head(errors)
plot(errors$iter, errors$train_rmse, col='red', xlab='Interaction', ylab='RMSE')
lines(errors$iter, errors$train_rmse, col='blue')
```
### check the # interaction for smallest test_rmse
```{r}
errors[errors$test_rmse==min(errors$test_rmse),]
```
### The final XGBoost model with nrounds=199
```{r}
xgb_model<-xgb.train(xgb_params, dtrain, nrounds=199)
```

### The importance of predictors

```{r}
imp<-xgb.importance(colnames(dtrain), model=xgb_model)
xgb.plot.importance(imp)
```

##Group 2
## Decision Tree

```{r}
target<-'price'
predictors<-c('carat', 'index_2','table', 'depth', 'cut_2', 'clarity_2', 'color_2')
## for loop to get MAE between maxdepth=1 andmaxdepth=10
for (i in 1:10){
  mae <- mae_values(maxdepth = i, target = target, predictors = predictors,
                  training_data = train, testing_data =test)
    print(glue::glue("Maxdepth: ",i,"\t MAE: ",mae))
}
  

```
## randomForest
### Do the similar steps as those done for group 1
```{r}

### fit a basic randomforest model, the default value of mtry=round(#number of predictors/3)
rdf.model<-randomForest(price~carat+index_2+table+depth+cut_2+clarity_2+color_2, ntree=50, data=train)
```
```{r}
## check the model details
rdf.model
```

```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```

### Use caret to tune the randomforest model
```{r}
target<-train$price
predictors<-train %>%
  select(c('carat', 'index_2','table', 'depth', 'cut_2', 'clarity_2','color_2'))%>%
  as.matrix()
tuned_rdf.model<-train(x=predictors, y=target, ntree=50, method='rf', data=train)
print(tuned_rdf.model)

```

### check the tuned radnomforest model with mtry= 4 and ntree=50
```{r}
rdf.model<-randomForest(price~carat+index_2+table+depth+cut_2+clarity_2+color_2, ntree=50, mtry=4,data=train)
```
```{r}
## check the model details
rdf.model
```
```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```


## Plot the importance of predictors
```{r}
imp<-importance(rdf.model)
imp_table<-data.frame(Variables=row.names(imp),imp_index=round(imp[, 'IncNodePurity'],5))

```
```{r imp_table, echo=FALSE}
ggplot(imp_table, aes(x = reorder(Variables, imp_index), 
                           y = imp_index, fill = imp_index)) +
  geom_bar(stat='identity') + 
  labs(x = 'Variables') +
  coord_flip() + 
  theme_classic()
```

## XGBoost modeling
```{r}

train_2_x<-train%>% select(-c('index_1','price','id','ln_carat','ln_price'))
train_2_x<-as.matrix(train_2_x)
test_2_x<-test%>%select(-c('index_1','price','id','ln_carat','ln_price'))
test_2_x<-as.matrix(test_2_x)
dtrain<-xgb.DMatrix(data=train_2_x, label=train$price)
dtest<-xgb.DMatrix(data=test_2_x, label=test$price)

```

### XGBoost parameters
```{r}
xgb_params<-list(colsample_bytree = 0.7,
                 subsample = 0.7,
                 booster = "gbtree",
                  max_depth = 5,
                 eta = 0.1,
                 eval_metric = "rmse",
                 objective = "reg:linear",
                  gamma = 0)
```

```{r}
watchlist<-list(train=dtrain, test=dtest)
```

### train xgboot model
```{r}
xgb_model<-xgb.train(xgb_params, dtrain, nrounds=200, watchlist=watchlist)
```

### plot train_RMSE/test_RMSE vs interaction

```{r}
# xgb_model ## call the model
errors<-data.frame(xgb_model$evaluation_log)
#head(errors)
plot(errors$iter, errors$train_rmse, col='red', xlab='Interaction', ylab='RMSE')
lines(errors$iter, errors$train_rmse, col='blue')
```
### check the # interaction for smallest test_rmse
```{r}
errors[errors$test_rmse==min(errors$test_rmse),]
```
### The final XGBoost model with nrounds=199
```{r}
xgb_model<-xgb.train(xgb_params, dtrain, nrounds=199)
```


### The importance of predictors

```{r}
imp<-xgb.importance(colnames(dtrain), model=xgb_model)
xgb.plot.importance(imp)
```

##Group 3
## Decision Tree

```{r}
target<-'ln_price'
predictors<-c('ln_carat', 'index_1','table', 'depth', 'cut_2', 'clarity_2', 'color_2')
## for loop to get MAE between maxdepth=1 andmaxdepth=10
for (i in 1:10){
  mae <- mae_values(maxdepth = i, target = target, predictors = predictors,
                  training_data = train, testing_data =test)
    print(glue::glue("Maxdepth: ",i,"\t MAE: ",mae))
}

```
```{r}
str(diamond)
```


## RandomForest
### Do the similar steps as those done for group 1
```{r}

### fit a basic randomforest model, the default value of mtry=round(#number of predictors/3)
rdf.model<-randomForest(ln_price~ln_carat+index_1+table+depth+cut_2+clarity_2+color_2, ntree=50, data=train)
```
```{r}
## check the model details
rdf.model
```

```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```


### Use caret to tune the randomforest model
```{r}
target<-train$ln_price
predictors<-train %>%
  select(c('ln_carat', 'index_1','table', 'depth', 'cut_2', 'clarity_2','color_2'))%>%
  as.matrix()
tuned_rdf.model<-train(x=predictors, y=target, ntree=50, method='rf', data=train)
print(tuned_rdf.model)

```

### check the tuned radnomforest model with mtry=4 and ntree=50
```{r}
rdf.model<-randomForest(ln_price~ln_carat+index_1+table+depth+cut_2+clarity_2+color_2, ntree=50, mtry=4,data=train)
```
```{r}
## check the model details
rdf.model
```
```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```


## Plot the importance of predictors
```{r}
imp<-importance(rdf.model)
imp_table<-data.frame(Variables=row.names(imp),imp_index=round(imp[, 'IncNodePurity'],5))

```
```{r imp_table, echo=FALSE}
ggplot(imp_table, aes(x = reorder(Variables, imp_index), 
                           y = imp_index, fill = imp_index)) +
  geom_bar(stat='identity') + 
  labs(x = 'Variables') +
  coord_flip() + 
  theme_classic()
```

## XGBoost modeling
```{r}

train_3_x<-train%>% select(-c('index_2','price','id','carat','ln_price'))
train_3_x<-as.matrix(train_3_x)
test_3_x<-test%>%select(-c('index_2','price','id','carat','ln_price'))
test_3_x<-as.matrix(test_3_x)
dtrain3<-xgb.DMatrix(data=train_3_x, label=train$ln_price)
dtest3<-xgb.DMatrix(data=test_3_x, label=test$ln_price)

```

### XGBoost parameters
```{r}
xgb_params<-list(colsample_bytree = 0.7,
                 subsample = 0.7,
                 booster = "gbtree",
                  max_depth = 5,
                 eta = 0.1,
                 eval_metric = "rmse",
                 objective = "reg:linear",
                  gamma = 0)
```

```{r}
watchlist<-list(train=dtrain, test=dtest3)
```

### train xgboot model
```{r}
xgb_model<-xgb.train(xgb_params, dtrain3, nrounds=200, watchlist=watchlist)
```

### plot train_RMSE/test_RMSE vs interaction

```{r}
# xgb_model ## call the model
errors<-data.frame(xgb_model$evaluation_log)
#head(errors)
plot(errors$iter, errors$train_rmse, col='red', xlab='Interaction', ylab='RMSE')
lines(errors$iter, errors$train_rmse, col='blue')
```
### check the # interaction for smallest test_rmse

```{r}
errors[errors$test_rmse==min(errors$test_rmse),]
```

### The final XGBoost model with nrounds=200
```{r}
xgb_model<-xgb.train(xgb_params, dtrain3, nrounds=200)
```

### The importance of predictors

```{r}
imp<-xgb.importance(colnames(dtrain3), model=xgb_model)
xgb.plot.importance(imp)
```


##Group 4
## Decision Tree

```{r}
target<-'ln_price'
predictors<-c('ln_carat', 'index_2','table', 'depth', 'cut_2', 'clarity_2', 'color_2')
## for loop to get MAE between maxdepth=1 andmaxdepth=10
for (i in 1:10){
  mae <- mae_values(maxdepth = i, target = target, predictors = predictors,
                  training_data = train, testing_data =test)
    print(glue::glue("Maxdepth: ",i,"\t MAE: ",mae))
}
  
```

## RandomForest
### Do the similar steps as those done for group 1
```{r}

### fit a basic randomforest model, the default value of mtry=round(#number of predictors/3)
rdf.model<-randomForest(ln_price~ln_carat+index_2+table+depth+cut_2+clarity_2+color_2, ntree=50, data=train)
```
```{r}
## check the model details
rdf.model
```

```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```


### Use caret to tune the randomforest model
```{r}
target<-train$ln_price
predictors<-train %>%
  select(c('ln_carat', 'index_2','table', 'depth', 'cut_2', 'clarity_2','color_2'))%>%
  as.matrix()
tuned_rdf.model<-train(x=predictors, y=target, ntree=50, method='rf', data=train)
print(tuned_rdf.model)

```

### check the tuned radnomforest model with mtry= 4and ntree=50
```{r}
set.seed(12345)
rdf.model<-randomForest(ln_price~ln_carat+index_2+table+depth+cut_2+clarity_2+color_2, ntree=50, mtry=4,data=train)
```
```{r}
## check the model details
rdf.model
```
```{r}
## check rmse (square root of mean squared error) and mae for the model on tetsing data
rmse<-rmse(rdf.model, test)
mae<-mae(rdf.model, test)
print(glue::glue("rdf.model: ", "\t RMSE : ", rmse, "\t MAE : ", mae))
```

## Plot the importance of predictors
```{r}
imp<-importance(rdf.model)
imp_table<-data.frame(Variables=row.names(imp),imp_index=round(imp[, 'IncNodePurity'],5))

```
```{r imp_table, echo=FALSE}
ggplot(imp_table, aes(x = reorder(Variables, imp_index), 
                           y = imp_index, fill = imp_index)) +
  geom_bar(stat='identity') + 
  labs(x = 'Variables') +
  coord_flip() + 
  theme_classic()
```

## XGBoost modeling
```{r}

train_4_x<-train%>% select(-c('index_1','price','id','carat','ln_price'))
train_4_x<-as.matrix(train_4_x)
test_4_x<-test%>%select(-c('index_1','price','id','carat','ln_price'))
test_4_x<-as.matrix(test_4_x)
dtrain4<-xgb.DMatrix(data=train_4_x, label=train$ln_price)
dtest4<-xgb.DMatrix(data=test_4_x, label=test$ln_price)

```

### XGBoost parameters
```{r}
xgb_params<-list(colsample_bytree = 0.7,
                 subsample = 0.7,
                 booster = "gbtree",
                  max_depth = 5,
                 eta = 0.1,
                 eval_metric = "rmse",
                 objective = "reg:linear",
                  gamma = 0)
```

```{r}
watchlist<-list(train=dtrain, test=dtest4)
```

### train xgboot model
```{r}
xgb_model<-xgb.train(xgb_params, dtrain4, nrounds=200, watchlist=watchlist)
```

### plot train_RMSE/test_RMSE vs interaction

```{r}
# xgb_model ## call the model
errors<-data.frame(xgb_model$evaluation_log)
#head(errors)
plot(errors$iter, errors$train_rmse, col='red', xlab='Interaction', ylab='RMSE')
lines(errors$iter, errors$test_rmse, col='blue')
```


### check the # interaction for smallest test_rmse

```{r}
errors[errors$test_rmse==min(errors$test_rmse),]
```

### The final XGBoost model with nrounds=200
```{r}
xgb_model<-xgb.train(xgb_params, dtrain4, nrounds=200)
```

### The importance of predictors

```{r}
imp<-xgb.importance(colnames(dtrain4), model=xgb_model)
xgb.plot.importance(imp)
```
#Summary the models

## Group1: randomforest model with mtry=4 and ntrees=50 has the lowest value of rmse
## Group2: randoforest model with mtry=4 and ntrees =50 has the lowest value of rmse
## Group3: xGBoost model with nrouds=200 has the lowest value of rmse
## Group4: XGBoost model with nrouns=200 has the lowest value of rmse



# Prediction of diamond price for new dataset

## load the dataset provided by client
```{r}
setwd("E://study//TAMU//685//practice using R")
diamond_new=read.csv("new_diamond.csv")

```

## data preprocessing and clean

```{r}
diamond_new<-diamond_new%>%
  select_if(is.numeric)
str(diamond_new)
```

## Group1, use randomforest model to predict index_1 in the new diamond dataset
### use randomfroest model to predict index_1

```{r}
##diamond_1<-diamond%>%
  select(-c('id','price','ln_price','index_2','ln_carat'))
train_1<-train%>%
  select(-c('id','price','ln_price','index_2','ln_carat'))


train_1$index_1 <-as.factor(train_1$index_1)
  
rdf<-randomForest(index_1~., ntree=100, mtry=3, data=train_1, importance=TRUE)
```

```{r}
### check the accurate 
test_1<-test%>%
  select(-c('id','price','ln_price','index_2','ln_carat'))
pred<-predict(rdf,test_1 )
mean(pred==test_1$index_1)
```
### use the model to predict index_1 for the  new diamond data set
```{r}
pred_index_1<-predict(rdf, diamond_new)
index_1<-as.numeric(levels(pred_index_1))[pred_index_1]
group1<-mutate(diamond_new, index_1)

head(group1)
```

```{r}
## The randomforest model for Group1
group1_model<-randomForest(price~carat+index_1+table+depth+cut_2+clarity_2+color_2, ntree=50, mtry=4,data=train)

```
```{r}
## predict the price for the new diamond dataset
price_group1<-predict(group1_model, newdata=group1)
## the prediction of total price
sum(price_group1)
```

## Group2, use randomforest model to predict index_2 in the new diamond dataset
### use randomfroest model to predict index_2

```{r}
##diamond_1<-diamond%>%select(-c('id','price','ln_price','index_2','ln_carat'))

train_2<-train%>%
  select(-c('id','price','ln_price','index_1','ln_carat'))


train_2$index_2 <-as.factor(train_2$index_2)
  
rdf<-randomForest(index_2~., ntree=100, mtry=3, data=train_2, importance=TRUE)
```

```{r}
### check the accurate 
test_2<-test%>%
  select(-c('id','price','ln_price','index_1','ln_carat'))
pred<-predict(rdf,test_2 )
mean(pred==test_2$index_2)
```
### use the model to predict index_1 for the  new diamond data set
```{r}
pred_index_2<-predict(rdf, diamond_new)
index_2<-as.numeric(levels(pred_index_2))[pred_index_2]

group2<-mutate(diamond_new, index_2)

str(group2)
```

```{r}
## The randomforest model for Group2
group2_model<-randomForest(price~carat+index_2+table+depth+cut_2+clarity_2+color_2, ntree=50, mtry=4,data=train)

```
```{r}
## predict the price for the new diamond dataset
price_group2<-predict(group2_model, newdata=group2)
## the prediction of total price
sum(price_group2)
```
## Group3, use XGBoost model to predict index_1 in the new diamond dataset
### use randomfroest model to predict index_1

```{r}

train_3<-train%>%
  select(-c('id','price','ln_price','index_2','carat'))


train_3$index_1 <-as.factor(train_3$index_1)
  
rdf<-randomForest(index_1~., ntree=100, mtry=3, data=train_3, importance=TRUE)
```

```{r}
### check the accurate 
test_3<-test%>%
  select(-c('id','price','ln_price','index_2','carat'))
pred<-predict(rdf,test_3 )
mean(pred==test_3$index_1)
```
### use the model to predict index_1 for the  new diamond data set
```{r}
pred_index_1<-predict(rdf, diamond_new)
index_1<-as.numeric(levels(pred_index_1))[pred_index_1]
group3<-mutate(diamond_new, index_1)

head(group3)

```

```{r}
## The XGBoost model for Group3
group3_model<-xgb.train(xgb_params, dtrain3, nrounds=200)

```
```{r}
## create xgb.matrix for xgboost prediction
ln_price<-rep(0, nrow(diamond_new))
group3<-mutate(group3,ln_price)


group3_x<-group3%>%select(-c('id','carat','ln_price'))
### should keep the same order of feautures as that in the train data which you used for xgb model
group3_x<-group3_x[,c('depth','table','cut_2','color_2','clarity_2','index_1','ln_carat')] 

group3_x<-as.matrix(group3_x)
Dgroup3<-xgb.DMatrix(data=group3_x, label=group3$ln_price)

```

```{r}
## predict the price for the new diamond dataset
ln_price_group3<-predict(group3_model, newdata=Dgroup3)

## the prediction of total price
sum(2^ln_price_group3)

```

## Group3, use XGBoost model to predict index_1 in the new diamond dataset
### use randomfroest model to predict index_1

```{r}

train_4<-train%>%
  select(-c('id','price','ln_price','index_1','carat'))


train_4$index_2 <-as.factor(train_4$index_2)
  
rdf<-randomForest(index_2~., ntree=100, mtry=3, data=train_4, importance=TRUE)
```

```{r}
### check the accurate 
test_4<-test%>%
  select(-c('id','price','ln_price','index_1','carat'))
pred<-predict(rdf,test_4 )
mean(pred==test_4$index_2)
```

### use the model to predict index21 for the  new diamond data set
```{r}
pred_index_1<-predict(rdf, diamond_new)
index_2<-as.numeric(levels(pred_index_2))[pred_index_2]
group4<-mutate(diamond_new, index_2)

head(group4)
```

```{r}
## The XGBoost model for Group3
group4_model<-xgb.train(xgb_params, dtrain4, nrounds=200)

```
```{r}
## create xgb.matrix for xgboost prediction
ln_price<-rep(0, nrow(diamond_new))
group4<-mutate(group4,ln_price)


group4_x<-group4%>%select(-c('id','carat','ln_price'))
### should keep the same order of feautures as that in the train data which you used for xgb model
group4_x<-group4_x[,c('depth','table','cut_2','color_2','clarity_2','index_2','ln_carat')] 

group4_x<-as.matrix(group4_x)
Dgroup4<-xgb.DMatrix(data=group4_x, label=group4$ln_price)

```

```{r}
## predict the price for the new diamond dataset
ln_price_group4<-predict(group4_model, newdata=Dgroup4)

## the prediction of total price
sum(2^ln_price_group4)

```

# Summary thr prediction
## Group 1  model used: randomforest        total price of the 5299 diamond: $20,424,530
## Group 2: model used: randomforest        total price of the 5299 diamond: $20,417,024
## Group 3: model used: XGBoost             total price of the 5299 diamond: $19,938,205
## Group 4: model used: XGBoost             total price of the 5299 diamond: $19,664,035